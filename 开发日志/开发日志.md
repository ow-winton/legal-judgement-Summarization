# 部署云的问题

1.evaluate方法没法正常使用，只能用回原版的load——metric方法，进而导致了缺失依赖，在虚拟环境中运行以下命令，安装缺失的依赖项 absl-py, nltk, 和 rouge-score解决（
需要吐槽的一点是load metric已经要过期不提供服务了，但是云服务器上的环境居然不支持，尽管尝试在云服务器上pip evaluate这个包，尽管显示成功安装，但是无法正常使用这个功能，最终不得已更换）
2. UnboundLocalError: local variable 'model' referenced before assignment 错误
原因：尝试加载了一个预训练的 BART 模型，但是在加载模型失败时并没有正确处理异常。因此，在模型加载失败的情况下，变量 model 并未成功赋值，但是后续仍然尝试对它进行配置，导致了 UnboundLocalError。

为了解决这个问题，你可以在模型加载失败时添加适当的异常处理代码，以确保程序能够继续执行或者给出适当的错误提示。例如，你可以添加一个 try-except 块来捕获加载模型时可能出现的异常，并在异常发生时打印错误信息或者执行其他处理操作

对这几个内容加了抛出异常后就正常运行了
```
    try:
        rouge = load_metric("rouge",trust_remote_code=True)
        print("Rouge evaluation module loaded successfully")
    except Exception as e:
        print(f"Error loading Rouge evaluation module: {e}")
    model_path = "/root/autodl-tmp/legal/models/fnlp-bart-base-chinese"

    try:
        tokenizer = BertTokenizer.from_pretrained(model_path)
        print("Tokenizer loaded successfully")
    except Exception as e:
        print(f"Error loading tokenizer: {e}")

    try:
        model = BartForConditionalGeneration.from_pretrained(model_path, use_cache=False)
        print("Model loaded successfully")
    except Exception as e:
        print(f"Error loading model: {e}")
```

3. 尽管进行了处理，但是还是会报错 ”TypeError: 'NoneType' object is not iterable“
   这是因为尝试使用 datasets 库的 rouge 指标时。错误指向 datasets.features.features.py 中的 encode_batch 方法，这表明在处理批次数据时，遇到了一个预期为列表或可迭代对象的参数，但实际上却是 None。
要解决这个问题，你需要检查在计算 ROUGE 指标时传递给 rouge.compute() 函数的 predictions 和 reference 参数。确保这些参数不是 None 且为可迭代的数据结构，比如列表。

更改compute方法如下后解决
   ```
def compute_metrics(pred):
    label_ids = pred.label_ids
    pred_ids = pred.predictions
    if label_ids is None or pred_ids is None:
        print("预测或参考文本为空，跳过此批次的ROUGE计算。")
        return {}
    pred_str = tokenizer.batch_decode(pred_ids,skip_special_tokens=True)
    label_ids[label_ids==-100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(label_ids,skip_special_tokens=True)
    try:
        rouge2_output = rouge.compute(predictions = pred_str,reference= label_str,rouge_types =["rouge2"])["rouge2"].mid


        return {
        "rouge2_precision": round(rouge2_output.precision, 4),
        "rouge2_recall": round(rouge2_output.recall, 4),
        "rouge2_fmeasure": round(rouge2_output.fmeasure, 4)
        }
    except Exception as e:
        print(f"在计算ROUGE分数时发生错误：{e}")
        return {}
   ```


5. 由于境内外墙的问题，我在境外写的代码可以直接使用hugging face的库，但是对于国内的云平台运行的时候没法使用一些hugging face上的库，比如bart-base-Chinese

   解决方法：在我的本地主机下载需要的库，然后上传到服务器上

   这个下载的过程也很奇怪，最开始下载只有三个文件成功下载

```
from transformers import BartTokenizer

# 指定模型名称
model_name = "fnlp/bart-base-chinese"

# 下载并加载模型
print(f"Downloading and loading tokenizer for model: {model_name}")
tokenizer = BartTokenizer.from_pretrained(model_name)

# 打印tokenizer内容确认下载成功
print(tokenizer)

# 保存模型到本地目录
save_directory = "/root/autodl-tmp/models/fnlp-bart-base-chinese"
tokenizer.save_pretrained(save_directory)
print(f"Tokenizer saved to: {save_directory}")


```
发现是因为没有用模型model = BertModel.from_pretrained(model_name)，加上更改后成功下载五个文件

   ```
   from transformers import BertTokenizer, BertModel

# 指定模型名称
model_name = "fnlp/bart-base-chinese"

# 下载并加载模型
print(f"Downloading and loading tokenizer and model for: {model_name}")
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# 打印tokenizer和model内容确认下载成功
print(tokenizer)
print(model)

# 保存tokenizer和model到本地目录
save_directory = "./models/fnlp-bart-base-chinese"
tokenizer.save_pretrained(save_directory)
model.save_pretrained(save_directory)
print(f"Tokenizer and model saved to: {save_directory}")

   ```

7. 一些路径问题，首先是服务器是ubantu系统，所以路径有时候没法读取
8. 第二个路径问题是代码写的时候是对于我的主机的绝对路径，但是服务器里面没法访问我的主机，所以会报错，最终检查了很久发现错误的路径，改成了服务器里的绝对路径解决了问题
9. 

# 知识点总结
1. 权重衰减（L2正则化）系数

    权重衰减（Weight Decay），也称为 L2 正则化，是一种防止模型过拟合的技术。它通过在损失函数中添加一个项来惩罚大权重，从而鼓励模型学习较小的权重
*2024年5月17日*
2. 学习率调度器类型

    学习率调度器（Learning Rate Scheduler）是一个调整学习率的方法，随着训练过程的进行，动态地调整学习率

    动态调整学习率可以帮助模型更快地收敛，并在接近最优点时减少学习率，防止在最优点附近震荡。
```commandline
余弦退火（Cosine Annealing）：学习率按照余弦函数逐渐减少。
指数衰减（Exponential Decay）：学习率按指数方式减少。
固定步长衰减（Step Decay）：每隔一定步数减少学习率。
```
3. 梯度累积步数

    梯度累积（Gradient Accumulation）是一种技术，它通过在多个步骤中累积梯度，然后一次性进行权重更新，来实现有效的较大批量训练

    当显存有限时，无法使用较大批量训练，这时可以通过累积多个小批量的梯度来模拟大批量训练，从而提高训练效率和稳定性。    

    gradient_accumulation_steps=4，则在每4个小批量后进行一次反向传播和权重更新。
    
4. 启用梯度检查点

    梯度检查点（Gradient Checkpointing）是一种技术，通过在前向传播过程中保存中间状态，减少内存使用

    在反向传播时动态计算部分前向传播结果，从而节省显存，适用于训练大模型或长序列。


# 随机种子
```commandline
def set_seed(seed: int = 3407):
    np.random.seed(seed)  # 设置 NumPy 的随机种子
    random.seed(seed)  # 设置 Python random 模块的随机种子
    torch.manual_seed(seed)  # 设置 PyTorch 的随机种子
    torch.cuda.manual_seed(seed)  # 设置 CUDA 的随机种子（如果使用 GPU）
    torch.backends.cudnn.deterministic = True  # 确保 CuDNN 使用确定性算法
    torch.backends.cudnn.benchmark = False  # 禁用 CuDNN 的自动优化
    os.environ["PYTHONHASHSEED"] = str(seed)  # 设置 Python 的哈希种子
    print(f"Random seed set as {seed}")

```
## 深层意义和原理
### 随机数生成器的初始状态：
随机数生成器（RNG）是一种算法，用于生成看似随机的数值序列。它实际上是伪随机的，因为这些数值序列是通过确定性算法生成的。
设置随机种子相当于初始化 RNG 的起始状态，使得从该状态开始生成的数值序列是确定的。
### 一致性和可重复性：
当你设置相同的随机种子并重复执行同一段代码时，RNG 会生成相同的数值序列。这对于实验的可重复性非常重要，因为它确保每次运行实验时，所有的随机操作（如数据分割、参数初始化等）都产生相同的结果。
### 多个 RNG 的协调：
在数据科学和机器学习中，可能会使用多个不同的库（如 NumPy、Python random 模块、PyTorch），每个库都有自己的 RNG。通过统一设置随机种子，可以确保所有库的随机操作都是一致的。
例如，NumPy、Python 的 random 模块和 PyTorch 都有各自的 RNG，通过分别设置它们的种子，可以确保这些库的随机操作同步。
# tokenizer
```commandline
BERT 和其他基于 Transformer 架构的预训练模型 如 BERT-base-chinese 使用的是 WordPiece 分词算法
```
1. 初始词汇表：

WordPiece 分词器最初包含一个基本词汇表，其中包含所有的单个字符（包括汉字）以及特殊标记（如 [CLS], [SEP] 等）。
2. 子词单元构建：

WordPiece 分词器将词汇表扩展为多个子词单元。子词单元是通过频繁合并在训练语料库中高频出现的子词对来构建的。例如，如果“国”出现得特别频繁，而“人民”也很频繁，那么“国”和“人民”可能会被合并成更大的单元。
3. 分词过程：

逐字分割：对于输入文本，分词器首先将其逐字符分割。

子词合并：分词器尝试将这些字符合并成最大的子词单元，同时确保这些单元在词汇表中存在。

标记序列生成：最终，输入文本被转换为子词单元序列，这些子词单元对应于模型词汇表中的标记。
# 模块化
今天尝试把项目进行模块化变成，把所有定义的函数统一保存在一个包里，项目中只调用函数来，以免所有内容集成在一起搞得项目很乱
# csv数据预处理
将前1000个数据拿出来进行开发，进行数据预处理 

预处理大概就是两个点
1. 把数据按照固定格式分成了输入以及我们的target部分。 这个的原因是中国法律文书基本严格按照（本院认为|
本院意见）把数据集分成了案件详细描述和案件的概述，那么我们要做的其实就是把数据分割成两个部分，一边是案件
详情，就是输入，另一边是案件的概述，是我们的目标
2. 第二个预处理就是对生成的csv文件先进行一个简单的处理，把空行和乱码数据清楚
空行用的是
```
    df = pd.read_csv(input_csv_path, encoding='utf-8')

    # 去除目标列 'target' 为空的数据
    df = df.dropna(subset=['target'])
```
3. 对于乱码，用的是正则表达式
一共试了好几个版本的正则表达式
```commandline
1. r'[^\u4e00-\u9fa5a-zA-Z0-9,.!?，。！？]'
2. r'[^\u4e00-\u9fa5a-zA-Z0-9,.!?，。！？（）《》“”\'\"\n]')
3. r'[^\u4e00-\u9fa5a-zA-Z0-9,.!?，。！？；：、“”‘’（）《》〈〉【】『』「」———\s]')

4. r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]'
```
第四个能够成功清理数据，前三个都导致输出的csv是空的列表

### 前三个正则表达式的问题：

前三个正则表达式匹配的是非指定字符集中的任意字符，这意味着只要字符串中包含一个不在指定字符集中的字符，就会被认为是乱码。
在中文文本中，可能会有一些常见但没有包括在这些字符集中的字符，比如特殊符号、罕见标点符号等，这导致正常文本被误认为包含乱码。
因此，前三个正则表达式过于严格，误将正常文本视为包含乱码，最终导致输出的 CSV 文件为空。

### 第四个正则表达式的成功：

第四个正则表达式匹配的是明确的不可见控制字符，这些字符通常确实不应出现在正常文本中。
这种方法更宽松，只过滤掉真正的乱码字符（即不可见控制字符），而不会误将正常文本视为乱码。
因此，第四个正则表达式能够成功清理数据。

# 最终清洗过的数据 
864个有效数据，可用率达到86.4%

*2024年5月17日*


# 平台兼容问题
1.  数据导入的问题 

最初写法是
```commandline
inputFile = open(file_path+'/'+file, encoding='utf8').readlines()
```
尽管对于python而言，数据路径是/又或者\都没有关系，但是还是要考虑到跨平台问题。

因此改成了以下写法
```commandline
 full_file_path = os.path.join(filepath, file)
 inputFile = open(full_file_path,encoding="utf-8").readlines()
```

# 编码问题
由于使用中文法律文书进行训练，所以编码一定要注意使用utf-8,这导致我的数据处理多次出现了问题

